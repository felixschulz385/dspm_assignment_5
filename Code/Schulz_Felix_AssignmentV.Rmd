---
title: 'Assignment V: GitHub and the ticketmaster.com API'
author: "Felix Schulz"
date: "12/26/2021"
output: 
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/felixschulz/OneDrive/Dokumente/Uni/Data Science Project Management 2021-2022/Assignment 5")
```

I hereby declare that this work and the code is solely my work. I exchanged 
ideas with fellow student Marvin Hoberg for this assignment.

## 1. Setting up a new GitHub repository

I set up a public github repo that is accessible using the link
[https://github.com/felixschulz385/dspm_assignment_5.git](https://github.com/felixschulz385/dspm_assignment_5.git)
and the account [https://github.com/felixschulz385](https://github.com/felixschulz385).

## 2. Getting to know the API

I retrieve the key from the API explorer and import the key from an external .R 
file for security reasons.

```{r}
source("Code/api_key.R")
```

There is a rate limit of five calls per second and 5000 calls per day. These 
will be enforced by my code.

## 3. Interacting with the API - the basics

The required packages are imported:

```{r, message = F}
# Core packages
library(httr)
library(jsonlite)
# Additional packages
library(tidyverse)
library(data.table)
library(janitor)
```

I also set up a list to contain API responses.

```{r}
responses = list()
```

Because I will make frequent calls to the API, I set up a function to wrap the
API call and simplify my code.

```{r}
ticketmaster_GET = function(endpoint, params = NULL, api_key){
  GET(paste0("https://app.ticketmaster.com/discovery/v2/",
             endpoint,
             ".json"),
      query = c(list("apikey" = api_key),
                params))
}
```

The first request to the API requests data from the *venues*-endpoint.

```{r}
responses$"venues_DE" = ticketmaster_GET("venues", 
                                         params = list("countryCode" = "DE"),
                                         api_key = api_key)
```

The response is then first parsed into a list.

```{r}
venue_data = 
  responses$"venues_DE" %>%
  # Retrieve the content as a string
  content(as = "text") %>%
  # Parse the json string
  fromJSON()
glimpse(venue_data)
```

The resulting list contains three main elements. First, there is an embedded
data frame of venues that is of central interest to us. The API also returns
links for retrieving other (first, next and last) pages of the data and
meta-information on the page and the total extent of the data.

Next, the information is gathered out of the embedded data frame and appropriate
data types are chosen.

```{r}
venue_data = 
  venue_data %>%
  # Get the resulting data frame
  pluck("_embedded", "venues") %>%
  # Retrieve the nested coordinates
  unnest(c(city, address, location), names_repair = "unique") %>%
  # Select the required columns
  select(name = "name...1", city = "name...10", postalCode, 
         address = "line1", url, longitude, latitude) %>%
  # Choose appropriate data types
  mutate(across(c(postalCode, longitude, latitude), as.double))
glimpse(venue_data)
```

## 4. Interacting with the API - advanced

All data in the *ticketmaster.com*-venues-API is accessed via pages that are
uniquely identified by their size and index. The maximum amount of rows 
retrievable from one function call appears to be 500. Therefore, I choose this 
page size to reduce the amount of calls to API as much as possible. Data is then
retrieved by looping through the page numbers until all results have been 
returned.

The following code generalizes a `for`-loop call to the API to ensure 
applicability to other parameters. First, the total amount of rows is retrieved.
Then, a blank list to store the output is created. Then, the loop collects the
parsed API results. These are then combined in a data frame.

```{r}
ticketmaster_GET_all_venues = function(params = NULL, api_key){
  ## Get the amount of total rows
  # Make an initial API call to get the total amount
  N_total_rows = 
    ticketmaster_GET("venues", 
                     c(params,
                          "size" = 1),
                     api_key) %>%
  # Retrieve the content as a string
  content(as = "text") %>%
  # Parse the json string
  fromJSON() %>%
  pluck("page", "totalElements")
  
  ## Create a blank list to store the output
  tmp = rep_len(NA, N_total_rows %/% 500 + 1) %>% as.list()
  
  ## Loop over the entries of the API
  for(i in seq_len(N_total_rows %/% 500 + 1)){
  tmp[[i]] = 
    # Make the API call
    ticketmaster_GET("venues", 
                     c(params,
                          "size" = 500,
                          "page" = (i - 1)),
                     api_key) %>%
    # Retrieve the content as a string
    content(as = "text") %>%
    # Parse the json string
    fromJSON() %>%
    # Get the resulting data frame
    pluck("_embedded", "venues") %>%
    # Retrieve the nested coordinates
    unnest(c(city, address, location), names_repair = "unique") %>%
    # Select the required columns
    select(name = "name...1", 
           # For different countries, different columns are returned.
           # The name-repair of unnest adds the column index to make
           # the names unique. This however results in different
           # column names for different countries. Therefore, I
           # implement this flexible approach.
           city = colnames(.)[str_detect(colnames(.), "name...") & 
                                colnames(.) != "name...1"], 
           postalCode, address = "line1", url, longitude, latitude) %>%
    # Choose appropriate data types
    mutate(across(c(postalCode, longitude, latitude), as.double))
  # Enforce the 5 calls/second rate limit
  Sys.sleep(.2)
  }
  return(tmp %>% rbindlist())
}
```

Testing this function, I call it to retrieve all German data. I include results 
from all locales -- not just the English ones -- to replicate the size of the 
table from the assignment's instructions. Further, I check for duplicates and 
remove them heuristically.

```{r}
ticketmaster_remove_dupe_venues = function(x){
  x %>%
    # Get the duplicate elements
    get_dupes(., c("name", "city", "address")) %>%
    # Select sorting categories
    group_by(name, city, address) %>%
    # Flag any where there is long and lat information available for one or
    # more entries
    mutate(flag_1 = any(!is.na(longitude) & !is.na(latitude))) %>%
    # Sort out those that miss long and lat information for one or more dupes
    filter(flag_1 & !is.na(longitude) & !is.na(latitude) |
             !flag_1) %>%
    select(-flag_1) %>%
    # Choose the first duplicate for each group
    slice_head(n = 1) %>%
    ungroup() %>%
    # Remove the dupes from the data
    anti_join(x %>% get_dupes(., c("name", "city", "address")),
              .) %>%
    anti_join(x, .)
}
```

```{r, message = F, warning = F}
# Run the query
venue_data = 
  ticketmaster_GET_all_venues(list("countryCode" = "DE",
                                   locale = "*"), api_key) %>%
  ticketmaster_remove_dupe_venues()
  
glimpse(venue_data)
```

## 5. Visualizing the extracted data

The following code adds the venues as points to the plot. Filtering of outliers
is done directly within creation of the plot.

```{r}
ggplot() + 
  # The polygons from the assignment's instructions
  geom_polygon(
    aes(x = long, y = lat, group = group), 
    data = map_data("world", region = "Germany"),
    fill = "grey90", color = "black") +
  # The points implemented by me
  geom_point(aes(x = longitude, y = latitude),
             # Data is filted to be inside Germany's bounding box
             data = 
               venue_data %>%
               filter(!is.na(longitude) & !is.na(latitude),
                      longitude > 5.866944 & longitude < 15.043611,
                      latitude > 47.271679 & latitude < 55.0846),
             alpha = .5) +
  # Theme choices from the assignment's instructions
  theme_void() + 
  coord_quickmap() +
  labs(title = "Event locations across Germany", 
       caption = "Source: ticketmaster.com") + 
  theme(title = element_text(size=8, face='bold'),
        plot.caption = element_text(face = "italic"))
```

However, I believe there is a much more comprehensive way of displaying the data.
Since 
`r (venue_data %>% filter(is.na(latitude) | is.na(longitude)) %>% nrow() / venue_data %>% nrow()) %>% round(2) %>% "*"(100)`% 
of all venues do not have coordinates assigned to them, one should display them
by their postal code which is available in 
`r (venue_data %>% filter(!is.na(postalCode)) %>% nrow() / venue_data %>% nrow()) %>% round(2) %>% "*"(100)`%
of cases in order to grasp the entirety of the data.

However, I believe that displaying the data on the postcode level is not 
feasible and does not produce easily comprehensible results. Therefore, I 
aggregate the data on the county level. I use an open-source database retrieved
from [opendatasoft.com](https://public.opendatasoft.com/explore/dataset/georef-germany-postleitzahl/information/)
to create a mapping of postcodes to counties. Since the data is licensed under
the *Open Database License*, I am allowed to modify and share the data.

Since the original file is 100MB large, I create simplified versions that I make 
accessible for download for replication purposes. This reduces the required
download from 100MB to less than 400KB. For future re-runs of this code, this 
step is therefore not run by default but the resulting table is downloaded 
instead.

```{r, eval = F}
if(!dir.exists("Data")) dir.create("Data")
if(!file.exists("Data/georef-germany-postleitzahl.csv")){
  download.file(
    url = "https://public.opendatasoft.com/explore/dataset/georef-germany-postleitzahl/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B",
    destfile = "Data/georef-germany-postleitzahl.csv")
}

  ## Import and clean the database of German postcodes
read_csv2("Data/georef-germany-postleitzahl.csv") %>%
  # Select a subset of columns
  summarise(postalCode = as.double(`Postleitzahl / Post code`),
            countyCode = `Kreis code`) %>%
  # Export
  write_csv("Data/correspondence_postCodes_counties.csv")
```

Additionally, I acquire a map of all German counties from the same source and
under the same license that I simplify and export.
([opendatasoft.com](https://public.opendatasoft.com/explore/dataset/georef-germany-kreis/information/)).

Both resulting files are then zipped and uploaded to a cloud hosting service.

```{r}
# Import necessary packages
library(sf)
```

```{r, eval = F}
# Import necessary packages
library(rmapshaper)

## Import and simplify the map
if(!dir.exists("Data")) dir.create("Data")
if(!dir.exists("Data/map_counties_Germany")) dir.create("Data/map_counties_Germany")
if(!dir.exists("Data/georef-germany-kreis")){
  download.file(
    url = "https://public.opendatasoft.com/explore/dataset/georef-germany-kreis/download/?format=shp&timezone=Europe/Berlin&lang=en",
    destfile = "Data/georef-germany-kreis.zip")
  unzip("Data/georef-germany-kreis.zip",
        exdir = "Data/georef-germany-kreis")
  file.remove("Data/georef-germany-kreis.zip")
}
## Import and clean the database of German counties
read_sf("Data/georef-germany-kreis/georef-germany-kreis-millesime.shp") %>%
  # Select a subset of columns
  select(countyCode = krs_code) %>%
  # Simplify the shapes
  ms_simplify() %>%
  # Export
  write_sf("Data/map_counties_Germany/map_counties_Germany.shp")
```

The following script creates the choropleth map of the number of venues in a
given county. The map and correspondence is first downloaded.

```{r, message = F}
## Download the files if necessary
if(!dir.exists("Data")) dir.create("Data")
if(!file.exists("Data/counties_Germany/counties_Germany.shp") |
   !file.exists("Data/postCodes_counties.csv")){
  download.file(
    url = "https://api.onedrive.com/v1.0/shares/s!AjXuK6Vn5knCiIIp2lUjEKLsZRoCig/root/content",
    destfile = "Data/Schulz_Felix_AssignmentV_add_Data.zip")
  unzip("Data/Schulz_Felix_AssignmentV_add_Data.zip",
        exdir = "Data")
  file.remove("Data/Schulz_Felix_AssignmentV_add_Data.zip")
}

## Create a shapefile/table to plot
map_dat = 
  venue_data %>%
  # Join with mapping between postcode and county
  left_join(read_csv("Data/correspondence_postCodes_counties.csv"), 
            by = "postalCode") %>%
  # Aggregate number of venues by county
  group_by(countyCode) %>%
  summarise(n_venues = n_distinct(name)) %>%
  ungroup() %>%
  # Join with map
  full_join(read_sf("Data/map_counties_Germany"), by = "countyCode") %>%
  # Set all missings to zero
  mutate(n_venues = ifelse(is.na(n_venues), 0, n_venues)) %>%
  # Make a simple feature object
  st_sf()

## Plot the choropleth map
ggplot() +
  # Plot the simple features
  geom_sf(aes(fill = n_venues + 1), 
          size = .1, data = map_dat) +
  # Scale the color of the map by the log number of venues
  # Make some adjustment to allow for values of zero
  scale_fill_viridis_c(trans = "log", 
                       breaks = c(2, 11, 101),
                       labels = ~ .x - 1) +
  # Theme choices from the assignment's instructions
  theme_void() +
  labs(title = "Density of event locations across Germany", 
       caption = "Source: ticketmaster.com",
       fill = "Number\nof\nvenues") + 
  theme(title = element_text(size=8, face='bold'),
        plot.caption = element_text(face = "italic"))
```

It turns out Bavarians either don't like ticketmaster.com or are severe couch 
potatoes.

## 6. Event locations in other countries

For this last task, I choose to download and plot all venues from the 
Netherlands. Since I standardize API call before, all I have to change is the
parameter of the *countryCode*. There are 2,073 venues listed on 
ticketmaster.com across the Netherlands.

```{r, message = F, warning = F}
venue_data_nl = 
  ticketmaster_GET_all_venues(list("countryCode" = "NL",
                                   "locale" = "*"), api_key) %>%
  ticketmaster_remove_dupe_venues()
  
glimpse(venue_data_nl)
```

The map can also largely be reproduced using the code from above. The only
significant change is the diversion from using a hardcoded bounding box to
algorithmically calculating it within the `ggplot` call.

```{r}
ggplot() + 
  # The polygons from the assignment's instructions
  geom_polygon(
    aes(x = long, y = lat, group = group), 
    data = map_data("world", region = "Netherlands"),
    fill = "grey90", color = "black") +
  # The points implemented by me
  geom_point(aes(x = longitude, y = latitude),
             data = 
               venue_data_nl %>%
               filter(!is.na(longitude) & !is.na(latitude),
                      longitude > 
                        map_data("world", region = "Netherlands") %>% 
                        pull(long) %>% min(), 
                      longitude < 
                        map_data("world", region = "Netherlands") %>% 
                        pull(long) %>% max(),
                      latitude > 
                        map_data("world", region = "Netherlands") %>% 
                        pull(lat) %>% min(), 
                      latitude < 
                        map_data("world", region = "Netherlands") %>% 
                        pull(lat) %>% max())) +
  # Theme choices from the assignment's instructions
  theme_void() + 
  coord_quickmap() +
  labs(title = "Event locations across the Netherlands", 
       caption = "Source: ticketmaster.com") + 
  theme(title = element_text(size=8, face='bold'),
        plot.caption = element_text(face = "italic"))
```

